{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.metrics import auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/thibault/Documents/oilprediction_challenge/Code/functions\")\n",
    "import skmice\n",
    "import functions as fn\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "PATH = \"/home/thibault/Documents/oilprediction_challenge/\"\n",
    "PATH_DATA = PATH + \"Data/\"\n",
    "PATH_RESULT = PATH + \"Result/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = pd.read_csv(PATH_DATA + \"Y_train.csv\", sep=\";\")\n",
    "X = pd.read_csv(PATH_DATA + \"Train.csv\", sep=\";\")\n",
    "final_test = pd.read_csv(PATH_DATA + \"Test.csv\", sep=\";\")\n",
    "\n",
    "X_full = pd.concat([X, final_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_0_in_lines = X.apply(lambda x: pd.isnull(x).sum(), axis=1)\n",
    "\n",
    "# Keep only line with less than n missing value\n",
    "X = X.loc[n_0_in_lines <= 20, :]\n",
    "Y = Y.loc[n_0_in_lines <= 20, :]\n",
    "\n",
    "X_full = pd.concat([X, final_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need for outliers handling using random Forest but we looked at it anyways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Create binary varialbe => note realli working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_transform = list(X_full.columns[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Variation in country vs variation in all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_transform_without_sum = [x for x in column_to_transform if ((\"Sum\" not in x) and (\"WTI\" not in x))]\n",
    "\n",
    "for var in column_to_transform_without_sum:\n",
    "    X_full.loc[:, var + \"_diff_with_global\"] =\\\n",
    "        X_full.loc[:, var] / X_full.loc[:, var.split(\"diff\")[0] + \"diffSum\" + var.split(\"diff\")[1]]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If positiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in column_to_transform:\n",
    "    X_full.loc[:, \"binary_if_positive_\" + var] =\\\n",
    "        X_full.loc[:, var].map(lambda x: fn.binary_if_positive(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Get columns with lot of 0\n",
    "columns_with_0 =\\\n",
    "    [ x for x in column_to_transform if (\"_diffClosing stocks(kmt)\" in x or \"_diffExports(kmt)\" in x\n",
    "                                        or \"_diffImports(kmt)\" in x or \"_diffRefinery intake(kmt\" in x) ]\n",
    "\n",
    "for var in columns_with_0:\n",
    "    X_full.loc[:, \"binary_if_0_\" + var] =\\\n",
    "        X_full.loc[:, var].map(lambda x: fn.binary_if_0(x))\n",
    "        \n",
    "binary_0_columns =\\\n",
    "    pd.Series(X_full.columns ).loc[pd.Series(X_full.columns ).map(lambda x: \"binary_if_0_\" in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Percent of 0 by columns, country and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in columns_with_0:\n",
    "    if var == columns_with_0[0]:\n",
    "        grouped_0_values =\\\n",
    "            X_full.loc[:, [var] + [\"country\", \"month\"]].groupby([\"country\", \"month\"])[var].\\\n",
    "                apply(lambda x: (x ==0).mean()).reset_index()\n",
    "    else:\n",
    "        temp_grouped_0_values =\\\n",
    "            X_full.loc[:, [var] + [\"country\", \"month\"]].groupby([\"country\", \"month\"])[var].\\\n",
    "                apply(lambda x: (x ==0).mean()).reset_index()\n",
    "        grouped_0_values = grouped_0_values.merge(temp_grouped_0_values, how=\"left\",\n",
    "                                                 on=[\"country\", \"month\"])\n",
    "        \n",
    "grouped_0_values.columns = list(grouped_0_values.columns[:2]) +\\\n",
    "                               [\"percent_0_\" + x for x in list(grouped_0_values.columns[2:])]\n",
    "    \n",
    "X_full = X_full.merge(grouped_0_values, how=\"left\", on=[\"country\", \"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binary if 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in column_to_transform:\n",
    "    X_full.loc[:, \"binary_if_null_\" + var] =\\\n",
    "        X_full.loc[:, var].map(lambda x: fn.binary_if_null(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate by month and country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:842: RuntimeWarning: Mean of empty slice\n",
      "  f = lambda x: func(x, *args, **kwargs)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "grouped_values =\\\n",
    "    X_full.loc[:, column_to_transform + [\"country\", \"month\"]].replace(0, np.nan).groupby([\"country\", \"month\"]).\\\n",
    "        agg([np.nansum, np.nanmean, np.nanstd, np.min, \n",
    "             np.max, np.nanmedian, fn.percentile(75),\n",
    "             fn.percentile(25), fn.percentile(10), fn.percentile(90),\n",
    "             fn.percentile(5), fn.percentile(95)]).reset_index()\n",
    "    \n",
    "grouped_values.columns = [' '.join(col).strip() for col in grouped_values.columns.values]\n",
    "\n",
    "X_full = X_full.merge(grouped_values, how=\"left\", on=[\"country\", \"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:842: RuntimeWarning: Mean of empty slice\n",
      "  f = lambda x: func(x, *args, **kwargs)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "grouped_values =\\\n",
    "    X_full.loc[:, column_to_transform + [\"country\"]].replace(0, np.nan).groupby([\"country\"]).\\\n",
    "        agg([np.nansum, np.nanmean, np.nanstd, np.min, \n",
    "             np.max, np.nanmedian, fn.percentile(75),\n",
    "             fn.percentile(25), fn.percentile(10), fn.percentile(90),\n",
    "             fn.percentile(5), fn.percentile(95)]).reset_index()\n",
    "    \n",
    "grouped_values.columns = [fn.new_name_if_not_id(' '.join(col).strip(), \"_country_\", \"country\") \n",
    "                          for col in grouped_values.columns.values]\n",
    "\n",
    "X_full = X_full.merge(grouped_values, how=\"left\", on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diff between export and import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of commercial balance for oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_import =\\\n",
    "    [ x for x in column_to_transform if (\"_diffImports(kmt)\" in x) ]\n",
    "    \n",
    "column_export =\\\n",
    "    [ x for x in column_to_transform if (\"_diffExports(kmt)\" in x) ]\n",
    "    \n",
    "columns_import_export = [(a,b) for a,b in zip(column_import, column_export)]\n",
    "\n",
    "for (a, b) in columns_import_export:\n",
    "    X_full.loc[:, a + b] = X_full.loc[:, a] - X_full.loc[:, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diff_import_export + closing stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of commercial balance + variation of stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_closing_stocks =\\\n",
    "    [ x for x in column_to_transform if (\"_diffClosing stocks(kmt)\" in x) ]\n",
    "    \n",
    "columns_import_export_closing_stock = [(a, b, c) for (a, b, c) in zip(column_import, column_export, columns_closing_stocks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (a, b, c) in columns_import_export_closing_stock:\n",
    "    X_full.loc[:, a + b + c] = X_full.loc[:, c] + (X_full.loc[:, a] - X_full.loc[:, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Countries with no variation in import and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_with_no_variation_in_import =\\\n",
    "    X_full.loc[:, [\"1_diffImports(kmt)\", \"country\"]].groupby(\"country\")[\"1_diffImports(kmt)\"].\\\n",
    "        apply(lambda x: (x == 0).mean()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_with_no_variation_in_export =\\\n",
    "    X_full.loc[:, [\"1_diffExports(kmt)\", \"country\"]].groupby(\"country\")[\"1_diffExports(kmt)\"].\\\n",
    "        apply(lambda x: (x == 0).mean()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_with_no_variation_in_import_export =\\\n",
    "    countries_with_no_variation_in_import.merge(countries_with_no_variation_in_export, how=\"left\",\n",
    "                                               on=\"country\")\n",
    "    \n",
    "countries_with_no_variation_in_import_export.loc[:, \"no_import_variation\"] =\\\n",
    "    countries_with_no_variation_in_import.loc[:, \"1_diffImports(kmt)\"] == 1\n",
    "\n",
    "countries_with_no_variation_in_import_export.loc[:, \"no_export_variation\"] =\\\n",
    "    countries_with_no_variation_in_export.loc[:, \"1_diffExports(kmt)\"] == 1\n",
    "    \n",
    "countries_with_no_variation_in_import_export.loc[:, \"no_export_nor_import_variation\"] =\\\n",
    "    countries_with_no_variation_in_import_export.no_import_variation &\\\n",
    "    countries_with_no_variation_in_import_export.no_export_variation\n",
    "    \n",
    "X_full =\\\n",
    "    X_full.merge(countries_with_no_variation_in_import_export, how=\"left\",\n",
    "                on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute relative value by countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns prefix to delete month in column name\n",
    "col_names_without_month = set([x.split(\"_\")[1] for x in column_to_transform])\n",
    "\n",
    "# By countries\n",
    "relative_value = pd.DataFrame()\n",
    "for var in list(col_names_without_month):\n",
    "    relative_value_by_countries = pd.DataFrame()\n",
    "    for localisation in set(X_full.country):\n",
    "        all_months_columns = [str(j) + \"_\" + var for j in range(1, 13)]\n",
    "        median_value = fn.data_frame_as_Series(X_full.loc[X_full.country == localisation, all_months_columns]).\\\n",
    "            dropna().sum()\n",
    "        relative_value_by_countries_columns =\\\n",
    "            X_full.loc[X_full.country == localisation, all_months_columns] / median_value\n",
    "        # replace inf\n",
    "        relative_value_by_countries_columns =\\\n",
    "            relative_value_by_countries_columns.replace([np.inf, -np.inf], np.nan)\n",
    "        relative_value_by_countries_columns.loc[:, \"ID\"] = X_full.loc[X_full.country == localisation, \"ID\"]\n",
    "        if relative_value_by_countries.empty:\n",
    "            relative_value_by_countries = relative_value_by_countries_columns\n",
    "        else:\n",
    "            relative_value_by_countries = pd.concat([relative_value_by_countries, \n",
    "                                                     relative_value_by_countries_columns],\n",
    "                                                   axis=0)\n",
    "    if relative_value.empty:\n",
    "        relative_value = relative_value_by_countries\n",
    "    else:\n",
    "        relative_value = relative_value.merge(relative_value_by_countries, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relative_value.columns = [fn.new_name_if_not_id(x, \"relative_by_countries_\") \n",
    "                          for x in list(relative_value.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full = X_full.merge(relative_value, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute relative values by countries and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# By countries\n",
    "relative_value = pd.DataFrame()\n",
    "for var in list(col_names_without_month):\n",
    "    relative_value_by_countries = pd.DataFrame()\n",
    "    for localisation in set(X_full.country):\n",
    "        for month in set(X_full.month):\n",
    "            all_months_columns = [str(j) + \"_\" + var for j in range(1, 13)]\n",
    "            median_value = fn.data_frame_as_Series(X_full.loc[(X_full.country == localisation) &\n",
    "                                                   (X_full.month == month), all_months_columns]).\\\n",
    "                dropna().sum()\n",
    "            relative_value_by_countries_columns =\\\n",
    "                X_full.loc[(X_full.country == localisation) &\n",
    "                           (X_full.month == month), all_months_columns] / median_value\n",
    "            # replace inf\n",
    "            relative_value_by_countries_columns =\\\n",
    "                relative_value_by_countries_columns.replace([np.inf, -np.inf], np.nan)\n",
    "            relative_value_by_countries_columns.loc[:, \"ID\"] = X_full.loc[(X_full.country == localisation) &\n",
    "                                                                          (X_full.month == month), \"ID\"]\n",
    "            if relative_value_by_countries.empty:\n",
    "                relative_value_by_countries = relative_value_by_countries_columns\n",
    "            else:\n",
    "                relative_value_by_countries = pd.concat([relative_value_by_countries, \n",
    "                                                         relative_value_by_countries_columns],\n",
    "                                                       axis=0)\n",
    "    if relative_value.empty:\n",
    "        relative_value = relative_value_by_countries\n",
    "    else:\n",
    "        relative_value = relative_value.merge(relative_value_by_countries, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "relative_value.columns = [fn.new_name_if_not_id(x, \"relative_by_countries_month_\") \n",
    "                          for x in list(relative_value.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full = X_full.merge(relative_value, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split again between test and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Transform month and country to categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We put only country as categorical because month seems to have higher impact as numeric\n",
    "X_full = pd.get_dummies(X_full, columns=[\"country\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_full.loc[X_full.ID.map(lambda x: x in list(X.ID)), :]\n",
    "final_test = X_full.loc[X_full.ID.map(lambda x: x in list(final_test.ID)), :]\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast model for feature enginerring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter_search = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.833522604821\n",
      "{'rf__max_features': 71, 'rf__min_samples_leaf': 17, 'rf__min_samples_split': 7, 'rf__n_estimators': 345}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "              \"rf__max_features\": sp_randint(1, 150),\n",
    "              \"rf__min_samples_split\": sp_randint(2, 30),\n",
    "              \"rf__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"rf__n_estimators\": sp_randint(20, 500)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"rf\", RandomForestClassifier())\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "rf_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.827095844925\n",
      "{'gb__max_features': 30, 'gb__min_samples_leaf': 7, 'gb__min_samples_split': 13, 'gb__n_estimators': 128}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "              \"gb__max_features\": sp_randint(1, 100),\n",
    "              \"gb__min_samples_split\": sp_randint(2, 30),\n",
    "              \"gb__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"gb__n_estimators\": sp_randint(20, 400)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"gb\", GradientBoostingClassifier())\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "gb_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Extra tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.83270537627\n",
      "{'et__max_features': 65, 'et__min_samples_leaf': 10, 'et__min_samples_split': 12, 'et__n_estimators': 309}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "              \"et__max_features\": sp_randint(1, 100),\n",
    "              \"et__min_samples_split\": sp_randint(2, 30),\n",
    "              \"et__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"et__n_estimators\": sp_randint(20, 400)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"et\", ExtraTreesClassifier())\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "et_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New model with feature from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_proba_predictions = rf_estimator.predict_proba(X_full.drop([\"ID\"], axis=1))[:, 1]\n",
    "gb_proba_predictions = gb_estimator.predict_proba(X_full.drop([\"ID\"], axis=1))[:, 1]\n",
    "et_proba_predictions = et_estimator.predict_proba(X_full.drop([\"ID\"], axis=1))[:, 1]\n",
    "\n",
    "\n",
    "X_full.loc[:, \"prediction_rf\"] = rf_proba_predictions\n",
    "X_full.loc[:, \"prediction_gb\"] = gb_proba_predictions\n",
    "X_full.loc[:, \"prediction_et\"] = gb_proba_predictions\n",
    "\n",
    "X = X_full.loc[X_full.ID.map(lambda x: x in list(X.ID)), :]\n",
    "final_test = X_full.loc[X_full.ID.map(lambda x: x in list(final_test.ID)), :]\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.879991013008\n",
      "{'pca__n_components': 292}\n"
     ]
    }
   ],
   "source": [
    "param_dist = {\n",
    "              # \"lr__max_features\": sp_randint(1, 100),\n",
    "              # \"lr__min_samples_split\": sp_randint(2, 30),\n",
    "              # \"lr__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              # \"lr__n_estimators\": sp_randint(20, 400)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"lr\", LogisticRegression(\"l1\"))\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "lr_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.92285970522176897"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predit = rf_estimator.predict_proba(X_train.drop([\"ID\", \"prediction_rf\", \n",
    "                                                          \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_train).merge(pd.DataFrame(y_train_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_train.Target, y_train_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82513436304653076"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predit = rf_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \n",
    "                                                        \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82481963879339548"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predit = gb_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \n",
    "                                                        \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Extra tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82413571878177505"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predit = et_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \n",
    "                                                        \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D) Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80111767459126204"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predit = lr_estimator.predict_proba(X_test.drop([\"ID\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### E) Combining estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.82704651463064283"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predit = 0.31 * rf_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \n",
    "                                                               \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1] +\\\n",
    "                0.31 * gb_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \n",
    "                                                               \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1] +\\\n",
    "                0.31 * et_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \n",
    "                                                               \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1] +\\\n",
    "                0.07 * lr_estimator.predict_proba(X_test.drop(\"ID\", axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see lot of features with importance < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([  3.52900000e+03,   1.14000000e+02,   3.10000000e+01,\n",
       "          1.00000000e+01,   2.20000000e+01,   1.30000000e+01,\n",
       "          4.00000000e+00,   0.00000000e+00,   1.00000000e+00,\n",
       "          1.00000000e+00]),\n",
       " array([ 0.        ,  0.00078177,  0.00156353,  0.0023453 ,  0.00312706,\n",
       "         0.00390883,  0.00469059,  0.00547236,  0.00625412,  0.00703589,\n",
       "         0.00781765]),\n",
       " <a list of 10 Patch objects>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAD8CAYAAACVZ8iyAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSRgGkiQMA0kShoEkCcNAkgT8D7g9ynRF/ozaAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fa9339e9be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(rf_estimator.named_steps[\"rf\"].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>importance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>1_diffSumRefinery intake(kmt) nanmedian</td>\n",
       "      <td>0.007818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>584</th>\n",
       "      <td>1_diffSumExports(kmt) percentile_75</td>\n",
       "      <td>0.006444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590</th>\n",
       "      <td>1_diffSumImports(kmt) nansum</td>\n",
       "      <td>0.005388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>583</th>\n",
       "      <td>1_diffSumExports(kmt) nanmedian</td>\n",
       "      <td>0.005179</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>358</th>\n",
       "      <td>percent_0_6_diffClosing stocks(kmt)</td>\n",
       "      <td>0.005178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1932</th>\n",
       "      <td>12_diffSumProduction(kmt) percentile_5</td>\n",
       "      <td>0.004784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>1_diffSumImports(kmt) percentile_10</td>\n",
       "      <td>0.004623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>589</th>\n",
       "      <td>1_diffSumExports(kmt) percentile_95</td>\n",
       "      <td>0.004543</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>1_diffSumClosing stocks(kmt) percentile_95</td>\n",
       "      <td>0.004538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>percent_0_8_diffClosing stocks(kmt)</td>\n",
       "      <td>0.004499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>percent_0_12_diffClosing stocks(kmt)</td>\n",
       "      <td>0.004300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>350</th>\n",
       "      <td>percent_0_4_diffClosing stocks(kmt)</td>\n",
       "      <td>0.004284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>1_diffSumImports(kmt) amax</td>\n",
       "      <td>0.004201</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>611</th>\n",
       "      <td>1_diffSumProduction(kmt) percentile_90</td>\n",
       "      <td>0.004163</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1522</th>\n",
       "      <td>9_diffWTI percentile_10</td>\n",
       "      <td>0.004126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        variable  importance\n",
       "619      1_diffSumRefinery intake(kmt) nanmedian    0.007818\n",
       "584          1_diffSumExports(kmt) percentile_75    0.006444\n",
       "590                 1_diffSumImports(kmt) nansum    0.005388\n",
       "583              1_diffSumExports(kmt) nanmedian    0.005179\n",
       "358          percent_0_6_diffClosing stocks(kmt)    0.005178\n",
       "1932      12_diffSumProduction(kmt) percentile_5    0.004784\n",
       "598          1_diffSumImports(kmt) percentile_10    0.004623\n",
       "589          1_diffSumExports(kmt) percentile_95    0.004543\n",
       "577   1_diffSumClosing stocks(kmt) percentile_95    0.004538\n",
       "366          percent_0_8_diffClosing stocks(kmt)    0.004499\n",
       "382         percent_0_12_diffClosing stocks(kmt)    0.004300\n",
       "350          percent_0_4_diffClosing stocks(kmt)    0.004284\n",
       "594                   1_diffSumImports(kmt) amax    0.004201\n",
       "611       1_diffSumProduction(kmt) percentile_90    0.004163\n",
       "1522                     9_diffWTI percentile_10    0.004126"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variable_importance =\\\n",
    "    pd.concat([pd.DataFrame(X_train.columns),\n",
    "               pd.DataFrame(rf_estimator.named_steps[\"rf\"].feature_importances_)], axis=1)\n",
    "variable_importance.columns =[\"variable\", \"importance\"]\n",
    "    \n",
    "variable_importance = variable_importance.sort_values(by=\"importance\", ascending=False)\n",
    "variable_importance.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_final_test = final_test.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_final_test = final_test.drop(\"ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/sklearn/linear_model/base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "predictions = 0.33 * rf_estimator.predict_proba(X_final_test.drop([\"prediction_rf\", \n",
    "                                                                   \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1] +\\\n",
    "              0.33 * gb_estimator.predict_proba(X_final_test.drop([\"prediction_rf\", \n",
    "                                                                   \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1] +\\\n",
    "              0.34 * et_estimator.predict_proba(X_final_test.drop([\"prediction_rf\", \n",
    "                                                                   \"prediction_gb\", \"prediction_et\"], axis=1))[:, 1] +\\\n",
    "              0.00 * lr_estimator.predict_proba(X_final_test)[:, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID10160</td>\n",
       "      <td>0.292571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10161</td>\n",
       "      <td>0.021153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID10162</td>\n",
       "      <td>0.424225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID10163</td>\n",
       "      <td>0.808301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID10164</td>\n",
       "      <td>0.245966</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    Target\n",
       "0  ID10160  0.292571\n",
       "1  ID10161  0.021153\n",
       "2  ID10162  0.424225\n",
       "3  ID10163  0.808301\n",
       "4  ID10164  0.245966"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(predictions).merge(pd.DataFrame(id_final_test).reset_index(drop=True), \n",
    "                                         left_index=True, right_index=True)\n",
    "result.columns = [\"Target\", \"ID\"]\n",
    "result = result.loc[:, [\"ID\", \"Target\"]]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result.to_csv(PATH_RESULT + \"result.csv\", index=False, sep=\";\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
