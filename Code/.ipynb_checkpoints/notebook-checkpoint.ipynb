{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline,FeatureUnion\n",
    "from sklearn.preprocessing import LabelEncoder, Imputer\n",
    "from sklearn.metrics import auc, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy as np\n",
    "import itertools as it\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "import os\n",
    "os.chdir(\"/home/thibault/Documents/oilprediction_challenge/Code/functions\")\n",
    "import skmice\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 200)\n",
    "\n",
    "PATH = \"/home/thibault/Documents/oilprediction_challenge/\"\n",
    "PATH_DATA = PATH + \"Data/\"\n",
    "PATH_RESULT = PATH + \"Result/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - LOAD DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y = pd.read_csv(PATH_DATA + \"Y_train.csv\", sep=\";\")\n",
    "X = pd.read_csv(PATH_DATA + \"Train.csv\", sep=\";\")\n",
    "final_test = pd.read_csv(PATH_DATA + \"Test.csv\", sep=\";\")\n",
    "\n",
    "X_full = pd.concat([X, final_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Data depending on month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_increase_by_month = X.merge(Y, how=\"left\", on=\"ID\").groupby(\"month\").Target.sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7f62137b0a90>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAbEtvuzWGkAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f62481c5518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(nb_increase_by_month.month, nb_increase_by_month.Target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Missing value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_0_in_lines = X.apply(lambda x: pd.isnull(x).sum(), axis=1)\n",
    "\n",
    "# Keep only line with less than n missing value\n",
    "X = X.loc[n_0_in_lines <= 20, :]\n",
    "Y = Y.loc[n_0_in_lines <= 20, :]\n",
    "\n",
    "X_full = pd.concat([X, final_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No need for outliers handling using random Forest but we will look at it anyways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We Create binary varialbe => note realli working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_transform = list(X_full.columns[3:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If positiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_if_positive(x):\n",
    "    if x > 0:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return(result)\n",
    "\n",
    "for var in column_to_transform:\n",
    "    X_full.loc[:, \"binary_if_positive_\" + var] =\\\n",
    "        X_full.loc[:, var].map(lambda x: binary_if_positive(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_if_0(x):\n",
    "    if x == 0:\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return(result)\n",
    "\n",
    "# Get columns with lot of 0\n",
    "columns_with_0 =\\\n",
    "    [ x for x in column_to_transform if (\"_diffClosing stocks(kmt)\" in x or \"_diffExports(kmt)\" in x\n",
    "                                        or \"_diffImports(kmt)\" in x or \"_diffRefinery intake(kmt\" in x) ]\n",
    "\n",
    "for var in columns_with_0:\n",
    "    X_full.loc[:, \"binary_if_0_\" + var] =\\\n",
    "        X_full.loc[:, var].map(lambda x: binary_if_0(x))\n",
    "        \n",
    "binary_0_columns =\\\n",
    "    pd.Series(X_full.columns ).loc[pd.Series(X_full.columns ).map(lambda x: \"binary_if_0_\" in x)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Percent of 0 by columns, country and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for var in columns_with_0:\n",
    "    if var == columns_with_0[0]:\n",
    "        grouped_0_values =\\\n",
    "            X_full.loc[:, [var] + [\"country\", \"month\"]].groupby([\"country\", \"month\"])[var].\\\n",
    "                apply(lambda x: (x ==0).mean()).reset_index()\n",
    "    else:\n",
    "        temp_grouped_0_values =\\\n",
    "            X_full.loc[:, [var] + [\"country\", \"month\"]].groupby([\"country\", \"month\"])[var].\\\n",
    "                apply(lambda x: (x ==0).mean()).reset_index()\n",
    "        grouped_0_values = grouped_0_values.merge(temp_grouped_0_values, how=\"left\",\n",
    "                                                 on=[\"country\", \"month\"])\n",
    "        \n",
    "grouped_0_values.columns = list(grouped_0_values.columns[:2]) +\\\n",
    "                               [\"percent_0_\" + x for x in list(grouped_0_values.columns[2:])]\n",
    "    \n",
    "X_full = X_full.merge(grouped_0_values, how=\"left\", on=[\"country\", \"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Binary if 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def binary_if_null(x):\n",
    "    if pd.isnull(x):\n",
    "        result = 1\n",
    "    else:\n",
    "        result = 0\n",
    "    return(result)\n",
    "\n",
    "for var in column_to_transform:\n",
    "    X_full.loc[:, \"binary_if_null_\" + var] =\\\n",
    "        X_full.loc[:, var].map(lambda x: binary_if_null(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate by month and country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def percentile(n):\n",
    "    def percentile_(x):\n",
    "        return np.nanpercentile(x, n)\n",
    "    percentile_.__name__ = 'percentile_%s' % n\n",
    "    return percentile_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:842: RuntimeWarning: Mean of empty slice\n",
      "  f = lambda x: func(x, *args, **kwargs)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "grouped_values =\\\n",
    "    X_full.loc[:, column_to_transform + [\"country\", \"month\"]].replace(0, np.nan).groupby([\"country\", \"month\"]).\\\n",
    "        agg([np.nansum, np.nanmean, np.nanstd, np.min, \n",
    "             np.max, np.nanmedian, percentile(75),\n",
    "             percentile(25), percentile(10), percentile(90),\n",
    "             percentile(5), percentile(95)]).reset_index()\n",
    "    \n",
    "grouped_values.columns = [' '.join(col).strip() for col in grouped_values.columns.values]\n",
    "\n",
    "X_full = X_full.merge(grouped_values, how=\"left\", on=[\"country\", \"month\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Aggregate by country"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def new_name_if_not_id(x, prefix_name=None, ID=\"ID\"):\n",
    "    if x != ID:\n",
    "        new_name = prefix_name + x\n",
    "    else:\n",
    "        new_name = x\n",
    "    return(new_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thibault/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py:842: RuntimeWarning: Mean of empty slice\n",
      "  f = lambda x: func(x, *args, **kwargs)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/nanfunctions.py:1423: RuntimeWarning: Degrees of freedom <= 0 for slice.\n",
      "  keepdims=keepdims)\n",
      "/home/thibault/anaconda3/lib/python3.6/site-packages/numpy/lib/function_base.py:3858: RuntimeWarning: All-NaN slice encountered\n",
      "  r = func(a, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "grouped_values =\\\n",
    "    X_full.loc[:, column_to_transform + [\"country\"]].replace(0, np.nan).groupby([\"country\"]).\\\n",
    "        agg([np.nansum, np.nanmean, np.nanstd, np.min, \n",
    "             np.max, np.nanmedian, percentile(75),\n",
    "             percentile(25), percentile(10), percentile(90),\n",
    "             percentile(5), percentile(95)]).reset_index()\n",
    "    \n",
    "grouped_values.columns = [new_name_if_not_id(' '.join(col).strip(), \"_country_\", \"country\") \n",
    "                          for col in grouped_values.columns.values]\n",
    "\n",
    "X_full = X_full.merge(grouped_values, how=\"left\", on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diff between export and import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of commercial balance for oil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_import =\\\n",
    "    [ x for x in column_to_transform if (\"_diffImports(kmt)\" in x) ]\n",
    "    \n",
    "column_export =\\\n",
    "    [ x for x in column_to_transform if (\"_diffExports(kmt)\" in x) ]\n",
    "    \n",
    "columns_import_export = [(a,b) for a,b in zip(column_import, column_export)]\n",
    "\n",
    "for (a, b) in columns_import_export:\n",
    "    X_full.loc[:, a + b] = X_full.loc[:, a] - X_full.loc[:, b]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Diff_import_export + closing stock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Variation of commercial balance + variation of stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "columns_closing_stocks =\\\n",
    "    [ x for x in column_to_transform if (\"_diffClosing stocks(kmt)\" in x) ]\n",
    "    \n",
    "columns_import_export_closing_stock = [(a, b, c) for (a, b, c) in zip(column_import, column_export, columns_closing_stocks)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for (a, b, c) in columns_import_export_closing_stock:\n",
    "    X_full.loc[:, a + b + c] = X_full.loc[:, c] + (X_full.loc[:, a] - X_full.loc[:, b])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Variation in country vs variation in all countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "column_to_transform_without_sum = [x for x in column_to_transform if (\"Sum\" not in x and \"WTI\" not in x)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1_diffClosing stocks(kmt)\n",
      "1_diffExports(kmt)\n",
      "1_diffImports(kmt)\n",
      "1_diffRefinery intake(kmt)\n",
      "2_diffClosing stocks(kmt)\n",
      "2_diffExports(kmt)\n",
      "2_diffImports(kmt)\n",
      "2_diffRefinery intake(kmt)\n",
      "3_diffClosing stocks(kmt)\n",
      "3_diffExports(kmt)\n"
     ]
    }
   ],
   "source": [
    "for var in column_to_transform_without_sum:\n",
    "    print(var)\n",
    "    X_full.loc[:, var + \"_diff_with_global\"] =\\\n",
    "        X_full.loc[:, var] / X_full.loc[:, var.split(\"diff\")[0] + \"diffSum\" + var.split(\"diff\")[1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Countries with no variation in import and export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_with_no_variation_in_import =\\\n",
    "    X_full.loc[:, [\"1_diffImports(kmt)\", \"country\"]].groupby(\"country\")[\"1_diffImports(kmt)\"].\\\n",
    "        apply(lambda x: (x == 0).mean()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_with_no_variation_in_export =\\\n",
    "    X_full.loc[:, [\"1_diffExports(kmt)\", \"country\"]].groupby(\"country\")[\"1_diffExports(kmt)\"].\\\n",
    "        apply(lambda x: (x == 0).mean()).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "countries_with_no_variation_in_import_export =\\\n",
    "    countries_with_no_variation_in_import.merge(countries_with_no_variation_in_export, how=\"left\",\n",
    "                                               on=\"country\")\n",
    "    \n",
    "countries_with_no_variation_in_import_export.loc[:, \"no_import_variation\"] =\\\n",
    "    countries_with_no_variation_in_import.loc[:, \"1_diffImports(kmt)\"] == 1\n",
    "\n",
    "countries_with_no_variation_in_import_export.loc[:, \"no_export_variation\"] =\\\n",
    "    countries_with_no_variation_in_export.loc[:, \"1_diffExports(kmt)\"] == 1\n",
    "    \n",
    "countries_with_no_variation_in_import_export.loc[:, \"no_export_nor_import_variation\"] =\\\n",
    "    countries_with_no_variation_in_import_export.no_import_variation &\\\n",
    "    countries_with_no_variation_in_import_export.no_export_variation\n",
    "    \n",
    "X_full =\\\n",
    "    X_full.merge(countries_with_no_variation_in_import_export, how=\"left\",\n",
    "                on=\"country\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute relative value by countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataframe as list\n",
    "def data_frame_as_Series(dataframe):\n",
    "    return(pd.Series(list(it.chain(*dataframe.values))))\n",
    "\n",
    "col_names_without_month = set([x.split(\"_\")[1] for x in column_to_transform])\n",
    "\n",
    "# By countries\n",
    "relative_value = pd.DataFrame()\n",
    "for var in list(col_names_without_month):\n",
    "    relative_value_by_countries = pd.DataFrame()\n",
    "    for localisation in set(X_full.country):\n",
    "        all_months_columns = [str(j) + \"_\" + var for j in range(1, 13)]\n",
    "        median_value = data_frame_as_Series(X_full.loc[X_full.country == localisation, all_months_columns]).\\\n",
    "            dropna().sum()\n",
    "        relative_value_by_countries_columns =\\\n",
    "            X_full.loc[X_full.country == localisation, all_months_columns] / median_value\n",
    "        # replace inf\n",
    "        relative_value_by_countries_columns =\\\n",
    "            relative_value_by_countries_columns.replace([np.inf, -np.inf], np.nan)\n",
    "        relative_value_by_countries_columns.loc[:, \"ID\"] = X_full.loc[X_full.country == localisation, \"ID\"]\n",
    "        if relative_value_by_countries.empty:\n",
    "            relative_value_by_countries = relative_value_by_countries_columns\n",
    "        else:\n",
    "            relative_value_by_countries = pd.concat([relative_value_by_countries, \n",
    "                                                     relative_value_by_countries_columns],\n",
    "                                                   axis=0)\n",
    "    if relative_value.empty:\n",
    "        relative_value = relative_value_by_countries\n",
    "    else:\n",
    "        relative_value = relative_value.merge(relative_value_by_countries, on=\"ID\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relative_value.columns = [new_name_if_not_id(x, \"relative_by_countries_\") for x in list(relative_value.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full = X_full.merge(relative_value, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Compute relative values by countries and month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Dataframe as list\n",
    "def data_frame_as_Series(dataframe):\n",
    "    return(pd.Series(list(it.chain(*dataframe.values))))\n",
    "\n",
    "col_names_without_month = set([x.split(\"_\")[1] for x in column_to_transform])\n",
    "\n",
    "# By countries\n",
    "relative_value = pd.DataFrame()\n",
    "for var in list(col_names_without_month):\n",
    "    relative_value_by_countries = pd.DataFrame()\n",
    "    for localisation in set(X_full.country):\n",
    "        for month in set(X_full.month):\n",
    "            all_months_columns = [str(j) + \"_\" + var for j in range(1, 13)]\n",
    "            median_value = data_frame_as_Series(X_full.loc[(X_full.country == localisation) &\n",
    "                                                           (X_full.month == month), all_months_columns]).\\\n",
    "                dropna().sum()\n",
    "            relative_value_by_countries_columns =\\\n",
    "                X_full.loc[(X_full.country == localisation) &\n",
    "                           (X_full.month == month), all_months_columns] / median_value\n",
    "            # replace inf\n",
    "            relative_value_by_countries_columns =\\\n",
    "                relative_value_by_countries_columns.replace([np.inf, -np.inf], np.nan)\n",
    "            relative_value_by_countries_columns.loc[:, \"ID\"] = X_full.loc[(X_full.country == localisation) &\n",
    "                                                                          (X_full.month == month), \"ID\"]\n",
    "            if relative_value_by_countries.empty:\n",
    "                relative_value_by_countries = relative_value_by_countries_columns\n",
    "            else:\n",
    "                relative_value_by_countries = pd.concat([relative_value_by_countries, \n",
    "                                                         relative_value_by_countries_columns],\n",
    "                                                       axis=0)\n",
    "    if relative_value.empty:\n",
    "        relative_value = relative_value_by_countries\n",
    "    else:\n",
    "        relative_value = relative_value.merge(relative_value_by_countries, on=\"ID\", how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "relative_value.columns = [new_name_if_not_id(x, \"relative_by_countries_month_\") for x in list(relative_value.columns)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_full = X_full.merge(relative_value, how=\"left\", on=\"ID\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split again between test and validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Transform month and country to categorical variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We put only country as categorical because month seems to have higher impact as numeric\n",
    "X_full = pd.get_dummies(X_full, columns=[\"country\"], drop_first=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Split dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = X_full.loc[X_full.ID.map(lambda x: x in list(X.ID)), :]\n",
    "final_test = X_full.loc[X_full.ID.map(lambda x: x in list(final_test.ID)), :]\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Custom transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import TransformerMixin, BaseEstimator\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "class Replace0(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value=0):\n",
    "        self.value=value\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        return X.replace(0, np.nan)\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "\n",
    "class ImputerWithGivenValue(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value=0):\n",
    "        self.value=value\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        return X.fillna(self.value)\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        return self\n",
    "    \n",
    "class ImputerWithMedianGroupby(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, value=0):\n",
    "        self.value=value\n",
    "        \n",
    "    def transform(self, X, *_):\n",
    "        return X.fillna(self.value)\n",
    "    \n",
    "    def fit(self, *_):\n",
    "        self.median_value = self.groupby([\"country\", \"month\"]).median()\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Main model with CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    \\nrf_selector = RandomForestClassifier(bootstrap=False,\\n                                    criterion=\"gini\",\\n                                    max_depth=None,\\n                                    max_features=9,\\n                                    min_samples_leaf=1,\\n                                    min_samples_split=10,\\n                                    n_estimators=198)\\n\\npipe = Pipeline([# (\"replace_0\", Replace0()),\\n                 (\"imputer\", ImputerWithGivenValue()),\\n                 (\"sd_scaler\", StandardScaler()),\\n                 # (\"pca\", PCA(whiten=True)),\\n                 (\"rf\", RandomForestClassifier(n_estimators=100, # criterion=\"gini\",\\n                                              max_depth=None))\\n                ])\\n                '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    \n",
    "rf_selector = RandomForestClassifier(bootstrap=False,\n",
    "                                    criterion=\"gini\",\n",
    "                                    max_depth=None,\n",
    "                                    max_features=9,\n",
    "                                    min_samples_leaf=1,\n",
    "                                    min_samples_split=10,\n",
    "                                    n_estimators=198)\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", ImputerWithGivenValue()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(whiten=True)),\n",
    "                 (\"rf\", RandomForestClassifier(n_estimators=100, # criterion=\"gini\",\n",
    "                                              max_depth=None))\n",
    "                ])\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipe2 = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", ImputerWithGivenValue()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(whiten=True)),\n",
    "                 ('feats', FeatureUnion([\n",
    "                    (\"rf\", RandomForestClassifier(max_depth=None)), # can pass in either a pipeline\n",
    "                    (\"gb\", GradientBoostingClassifier(max_depth=None)) # or a transformer\n",
    "                    ])),\n",
    "                 (\"lr\", LogisticRegression())\n",
    "                ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.749254764806\n"
     ]
    }
   ],
   "source": [
    "# specify parameters and distributions to sample from\n",
    "param_dist = {\n",
    "              \"feats__rf__max_features\": sp_randint(1, 100),\n",
    "              \"feats__rf__min_samples_split\": sp_randint(2, 30),\n",
    "              \"feats__rf__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"feats__rf__n_estimators\": sp_randint(150, 600),\n",
    "              \"feats__gb__max_features\": sp_randint(1, 100),\n",
    "              \"feats__gb__min_samples_split\": sp_randint(2, 30),\n",
    "              \"feats__gb__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"feats__gb__n_estimators\": sp_randint(150, 300)\n",
    "             }\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 4\n",
    "random_search = RandomizedSearchCV(pipe2, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "estimator = random_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'feats__gb__max_features': 74,\n",
       " 'feats__gb__min_samples_leaf': 8,\n",
       " 'feats__gb__min_samples_split': 16,\n",
       " 'feats__gb__n_estimators': 189,\n",
       " 'feats__rf__max_features': 69,\n",
       " 'feats__rf__min_samples_leaf': 18,\n",
       " 'feats__rf__min_samples_split': 8,\n",
       " 'feats__rf__n_estimators': 451}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast model for feature enginerring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_iter_search = 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "              \"rf__max_features\": sp_randint(1, 100),\n",
    "              \"rf__min_samples_split\": sp_randint(2, 30),\n",
    "              \"rf__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"rf__n_estimators\": sp_randint(20, 400)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"rf\", RandomForestClassifier())\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "rf_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Gradient boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "              \"gb__max_features\": sp_randint(1, 100),\n",
    "              \"gb__min_samples_split\": sp_randint(2, 30),\n",
    "              \"gb__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"gb__n_estimators\": sp_randint(20, 400)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"gb\", GradientBoostingClassifier())\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "gb_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### New model with feature from previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rf_proba_predictions = rf_estimator.predict_proba(X_full.drop([\"ID\"], axis=1))[:, 1]\n",
    "gb_proba_predictions = gb_estimator.predict_proba(X_full.drop([\"ID\"], axis=1))[:, 1]\n",
    "\n",
    "\n",
    "X_full.loc[:, \"prediction_rf\"] = rf_proba_predictions\n",
    "X_full.loc[:, \"prediction_gb\"] = gb_proba_predictions\n",
    "\n",
    "X = X_full.loc[X_full.ID.map(lambda x: x in list(X.ID)), :]\n",
    "final_test = X_full.loc[X_full.ID.map(lambda x: x in list(final_test.ID)), :]\n",
    "\n",
    "X_train, X_test, y_train, y_test =\\\n",
    "    train_test_split(X, Y, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "param_dist = {\n",
    "              \"et__max_features\": sp_randint(1, 100),\n",
    "              \"et__min_samples_split\": sp_randint(2, 30),\n",
    "              \"et__min_samples_leaf\": sp_randint(1, 20),\n",
    "              # \"rf__bootstrap\": [False],\n",
    "              # \"pca__n_components\": sp_randint(10, 300),\n",
    "              # \"pca__whiten\": [True, False],\n",
    "              # \"select_from_model__threshold\":sp_randint(0, 0.01),\n",
    "              \"et__n_estimators\": sp_randint(20, 400)\n",
    "             }\n",
    "\n",
    "pipe = Pipeline([# (\"replace_0\", Replace0()),\n",
    "                 (\"imputer\", Imputer()),\n",
    "                 (\"sd_scaler\", StandardScaler()),\n",
    "                 # (\"pca\", PCA(n_components=150, whiten=True)),\n",
    "                 (\"et\", ExtraTreesClassifier())\n",
    "                 # (\"rf\", LogisticRegression(penalty=\"l1\"))\n",
    "                ])\n",
    "\n",
    "random_search = RandomizedSearchCV(pipe, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search, n_jobs=4, scoring=\"roc_auc\")\n",
    "\n",
    "random_search.fit(X_train.drop(\"ID\", axis=1), y_train.Target)\n",
    "\n",
    "print(random_search.best_score_)\n",
    "\n",
    "et_estimator = random_search.best_estimator_\n",
    "\n",
    "print(random_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 - EVALUATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A) Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Train set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_train_predit = rf_estimator.predict_proba(X_train.drop([\"ID\", \"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_train).merge(pd.DataFrame(y_train_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_train.Target, y_train_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_test_predit = rf_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B) Gradient boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_predit = gb_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Extra tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_predit = et_estimator.predict_proba(X_test.drop(\"ID\", axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C) Combining estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test_predit = 0.4 * rf_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1] +\\\n",
    "                0.4 * gb_estimator.predict_proba(X_test.drop([\"ID\", \"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1] +\\\n",
    "                0.2 * et_estimator.predict_proba(X_test.drop(\"ID\", axis=1))[:, 1]\n",
    "\n",
    "result_comparison = pd.DataFrame(y_test).merge(pd.DataFrame(y_test_predit), left_index=True, right_index=True)\n",
    "result_comparison.sample(10)\n",
    "\n",
    "roc_auc_score(y_test.Target, y_test_predit)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see lot of features with importance < 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.hist(rf_estimator.named_steps[\"rf\"].feature_importances_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "variable_importance =\\\n",
    "    pd.concat([pd.DataFrame(X_train.columns),\n",
    "               pd.DataFrame(rf_estimator.named_steps[\"rf\"].feature_importances_)], axis=1)\n",
    "variable_importance.columns =[\"variable\", \"importance\"]\n",
    "    \n",
    "variable_importance = variable_importance.sort_values(by=\"importance\", ascending=False)\n",
    "variable_importance.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 - Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "id_final_test = final_test.ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_final_test = final_test.drop(\"ID\", axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions =\\\n",
    "    0.4 * rf_estimator.predict_proba(X_final_test.drop([\"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1] +\\\n",
    "    0.4 * gb_estimator.predict_proba(X_final_test.drop([\"prediction_rf\", \"prediction_gb\"], axis=1))[:, 1] +\\\n",
    "    0.2 * et_estimator.predict_proba(X_final_test)[:, 1]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID10160</td>\n",
       "      <td>0.326636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID10161</td>\n",
       "      <td>0.029988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID10162</td>\n",
       "      <td>0.363656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID10163</td>\n",
       "      <td>0.861999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID10164</td>\n",
       "      <td>0.254305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        ID    Target\n",
       "0  ID10160  0.326636\n",
       "1  ID10161  0.029988\n",
       "2  ID10162  0.363656\n",
       "3  ID10163  0.861999\n",
       "4  ID10164  0.254305"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(predictions).merge(pd.DataFrame(id_final_test).reset_index(drop=True), \n",
    "                                         left_index=True, right_index=True)\n",
    "result.columns = [\"Target\", \"ID\"]\n",
    "result = result.loc[:, [\"ID\", \"Target\"]]\n",
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "result.to_csv(PATH_RESULT + \"result.csv\", index=False, sep=\";\", quoting=csv.QUOTE_NONNUMERIC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
